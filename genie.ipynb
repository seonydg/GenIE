{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mRN3Wt9EwcFj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22409,"status":"ok","timestamp":1714124123125,"user":{"displayName":"seonyong","userId":"18015838499630991285"},"user_tz":-540},"id":"IBS_CNTawgMF","outputId":"cbe478cf-fce4-43fd-af0e-4ba5ec3ae85f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104721,"status":"ok","timestamp":1714124232713,"user":{"displayName":"seonyong","userId":"18015838499630991285"},"user_tz":-540},"id":"FyaTTH8PwcFm","outputId":"8f190a3e-a233-4f37-874c-7e5523dc060c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Collecting datasets\n","  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Collecting huggingface-hub>=0.21.2 (from datasets)\n","  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n"]}],"source":["!pip install torch\n","!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["6d196e6b2c46468caf90411fb892795f","5591caf03ccb4e6caed4e3d193ce1ae4","23166b98c7434cc0b92ff0b76c9e55f5","3bbec00911c8456ea250e3ed18e735d1","02090842a8844674b3688ad0e15cfd30","84b2fa9a0a4049aeb4d90baeb1cd6b47","a61ffd51471f434f821c0d3b459f011e","876192d0308e4a7da22eb2922d98e360","10f850375f6848d1b74e29563d9f5fd0","f78d1274d93b4a3a8bca7a4d67c0df60","f72dcf6b56994814897c951f2ed7afc4","8718b7f80f774f96bfccd64a94a4baee","3ee48a4bbcc9470ea223b7c4601759bf","2652885e6a23453f8079e721bcb89ebd","eb145bec42f54dec83ea2ac4b556a477","ee7617e52fdf482996972821edd749b9","e9f49313698944a7acd149954708e786","0663415a40a94c25b6ec736abc3c0130","30dbef9518c442618f9bce896b4305e7","528331e3931d4a95af92d18657a034ab","c544a43227064d4280870b5936a9cb62","d5d557a5e36a4a0c896d420230fe645d"]},"executionInfo":{"elapsed":2471,"status":"ok","timestamp":1714124451841,"user":{"displayName":"seonyong","userId":"18015838499630991285"},"user_tz":-540},"id":"BN8kR1K9wcFw","outputId":"9daff729-ee15-4318-a8b1-1f742504cca9"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d196e6b2c46468caf90411fb892795f","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/2.30M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8718b7f80f774f96bfccd64a94a4baee","version_major":2,"version_minor":0},"text/plain":["Generating full split:   0%|          | 0/14237 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","data = load_dataset(\"web_nlg\", 'release_v1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nduj4U5Kw-Yz"},"outputs":[],"source":["# dataset -> WN18RR, FB15-237K"]},{"cell_type":"markdown","metadata":{"id":"dHZ8hw-HyGLl"},"source":["# Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bfz8huceyF6O"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmh5QA7lyF2S"},"outputs":[],"source":["class TransE(Model):\n","\tdef __init__(self, ent_tot, rel_tot, dim = 100, p_norm = 1, norm_flag = True, margin = None, epsilon = None):\n","\t\tsuper(TransE, self).__init__(ent_tot, rel_tot)\n","\t\t\n","\t\tself.dim = dim\n","\t\tself.margin = margin\n","\t\tself.epsilon = epsilon\n","\t\tself.norm_flag = norm_flag\n","\t\tself.p_norm = p_norm\n","\n","\t\tself.ent_embeddings = nn.Embedding(self.ent_tot, self.dim)\n","\t\tself.rel_embeddings = nn.Embedding(self.rel_tot, self.dim)\n","\n","\t\tif margin == None or epsilon == None:\n","\t\t\tnn.init.xavier_uniform_(self.ent_embeddings.weight.data)\n","\t\t\tnn.init.xavier_uniform_(self.rel_embeddings.weight.data)\n","\t\telse:\n","\t\t\tself.embedding_range = nn.Parameter(\n","\t\t\t\ttorch.Tensor([(self.margin + self.epsilon) / self.dim]), requires_grad=False\n","\t\t\t\t)\n","\n","\t\t\tnn.init.uniform_(\n","\t\t\t\ttensor = self.ent_embeddings.weight.data, \n","\t\t\t\ta = -self.embedding_range.item(), \n","\t\t\t\tb = self.embedding_range.item()\n","\t\t\t\t)\n","\n","\t\t\tnn.init.uniform_(\n","\t\t\t\ttensor = self.rel_embeddings.weight.data, \n","\t\t\t\ta= -self.embedding_range.item(), \n","\t\t\t\tb= self.embedding_range.item()\n","\t\t\t\t)\n","\n","\t\tif margin != None:\n","\t\t\tself.margin = nn.Parameter(torch.Tensor([margin]))\n","\t\t\tself.margin.requires_grad = False\n","\t\t\tself.margin_flag = True\n","\t\telse:\n","\t\t\tself.margin_flag = False\n","\n","\n","\tdef _calc(self, h, t, r, mode):\n","\t\tif self.norm_flag:\n","\t\t\th = F.normalize(h, 2, -1)\n","\t\t\tr = F.normalize(r, 2, -1)\n","\t\t\tt = F.normalize(t, 2, -1)\n","\t\tif mode != 'normal':\n","\t\t\th = h.view(-1, r.shape[0], h.shape[-1])\n","\t\t\tt = t.view(-1, r.shape[0], t.shape[-1])\n","\t\t\tr = r.view(-1, r.shape[0], r.shape[-1])\n","\t\tif mode == 'head_batch':\n","\t\t\tscore = h + (r - t)\n","\t\telse:\n","\t\t\tscore = (h + r) - t\n","\t\tscore = torch.norm(score, self.p_norm, -1).flatten()\n","\t\t\n","\t\treturn score\n","\n","\n","\tdef forward(self, data):\n","\t\tbatch_h = data['batch_h']\n","\t\tbatch_t = data['batch_t']\n","\t\tbatch_r = data['batch_r']\n","\t\tmode = data['mode']\n","\t\th = self.ent_embeddings(batch_h)\n","\t\tt = self.ent_embeddings(batch_t) #bs x embedding dim\n","\t\tr = self.rel_embeddings(batch_r)\n","\t\tscore = self._calc(h ,t, r, mode)\n","\t\t\n","    #margin ranking loss\n","\t\tif self.margin_flag:\n","\t\t\treturn self.margin - score\n","\t\telse:\n","\t\t\treturn score\n","\n","\n","\tdef regularization(self, data):\n","\t\tbatch_h = data['batch_h']\n","\t\tbatch_t = data['batch_t']\n","\t\tbatch_r = data['batch_r']\n","\t\th = self.ent_embeddings(batch_h)\n","\t\tt = self.ent_embeddings(batch_t)\n","\t\tr = self.rel_embeddings(batch_r)\n","\t\tregul = (torch.mean(h ** 2) + \n","\t\t\t\t torch.mean(t ** 2) + \n","\t\t\t\t torch.mean(r ** 2)) / 3\n","\t\t\n","\t\treturn regul\n","\n","\n","\tdef predict(self, data):\n","\t\tscore = self.forward(data)\n","\t\tif self.margin_flag:\n","\t\t\tscore = self.margin - score\n","\t\t\treturn score.cpu().data.numpy()\n","\t\telse:\n","\t\t\treturn score.cpu().data.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# embedding lookup -> triple score calc (model by model) -> return, 이과정에서 vector 들이 잘 학습됌."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from .Model import Model # ?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DistMult(Model):\n","\tdef __init__(self, ent_tot, rel_tot, dim = 100, margin = None, epsilon = None):\n","\t\tsuper(DistMult, self).__init__(ent_tot, rel_tot)\n","\n","\t\tself.dim = dim\n","\t\tself.margin = margin\n","\t\tself.epsilon = epsilon\n","\t\tself.ent_embeddings = nn.Embedding(self.ent_tot, self.dim)\n","\t\tself.rel_embeddings = nn.Embedding(self.rel_tot, self.dim)\n","\n","\t\tif margin == None or epsilon == None:\n","\t\t\tnn.init.xavier_uniform_(self.ent_embeddings.weight.data)\n","\t\t\tnn.init.xavier_uniform_(self.rel_embeddings.weight.data)\n","\t\telse:\n","\t\t\tself.embedding_range = nn.Parameter(\n","\t\t\t\ttorch.Tensor([(self.margin + self.epsilon) / self.dim]), requires_grad=False\n","\t\t\t)\n","\t\t\tnn.init.uniform_(\n","\t\t\t\ttensor = self.ent_embeddings.weight.data, \n","\t\t\t\ta = -self.embedding_range.item(), \n","\t\t\t\tb = self.embedding_range.item()\n","\t\t\t)\n","\t\t\tnn.init.uniform_(\n","\t\t\t\ttensor = self.rel_embeddings.weight.data, \n","\t\t\t\ta= -self.embedding_range.item(), \n","\t\t\t\tb= self.embedding_range.item()\n","\t\t\t)\n","\n","\n","\tdef _calc(self, h, t, r, mode):\n","\t\tif mode != 'normal':\n","\t\t\th = h.view(-1, r.shape[0], h.shape[-1])\n","\t\t\tt = t.view(-1, r.shape[0], t.shape[-1])\n","\t\t\tr = r.view(-1, r.shape[0], r.shape[-1])\n","\t\tif mode == 'head_batch':\n","\t\t\tscore = h * (r * t)\n","\t\telse:\n","\t\t\tscore = (h * r) * t\n","\t\tscore = torch.sum(score, -1).flatten()\n","\n","\t\treturn score\n","\n","\n","\tdef forward(self, data):\n","\t\tbatch_h = data['batch_h']\n","\t\tbatch_t = data['batch_t']\n","\t\tbatch_r = data['batch_r']\n","\t\tmode = data['mode']\n","\t\th = self.ent_embeddings(batch_h)\n","\t\tt = self.ent_embeddings(batch_t)\n","\t\tr = self.rel_embeddings(batch_r)\n","\t\tscore = self._calc(h ,t, r, mode)\n","\n","\t\treturn score\n","\n","\n","\tdef regularization(self, data):\n","\t\tbatch_h = data['batch_h']\n","\t\tbatch_t = data['batch_t']\n","\t\tbatch_r = data['batch_r']\n","\n","\t\th = self.ent_embeddings(batch_h)\n","\t\tt = self.ent_embeddings(batch_t)\n","\t\tr = self.rel_embeddings(batch_r)\n","\t\tregul = (torch.mean(h ** 2) + torch.mean(t ** 2) + torch.mean(r ** 2)) / 3\n","\t\t\n","\t\treturn regul\n","\n","\n","\tdef l3_regularization(self):\n","\t\treturn (self.ent_embeddings.weight.norm(p = 3)**3 + self.rel_embeddings.weight.norm(p = 3)**3)\n","\n","\n","\tdef predict(self, data):\n","\t\tscore = -self.forward(data)\n","\n","\t\treturn score.cpu().data.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from .Model import Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# complex space -> real dim, imaginary dim\n","class ComplEx(Model):\n","    def __init__(self, ent_tot, rel_tot, dim = 100):\n","        super(ComplEx, self).__init__(ent_tot, rel_tot)\n","\n","        self.dim = dim\n","        self.ent_re_embeddings = nn.Embedding(self.ent_tot, self.dim)\n","        self.ent_im_embeddings = nn.Embedding(self.ent_tot, self.dim)\n","        self.rel_re_embeddings = nn.Embedding(self.rel_tot, self.dim)\n","        self.rel_im_embeddings = nn.Embedding(self.rel_tot, self.dim)\n","\n","        nn.init.xavier_uniform_(self.ent_re_embeddings.weight.data)\n","        nn.init.xavier_uniform_(self.ent_im_embeddings.weight.data)\n","        nn.init.xavier_uniform_(self.rel_re_embeddings.weight.data)\n","        nn.init.xavier_uniform_(self.rel_im_embeddings.weight.data)\n","\n","    # (a+bi)(c+di)\n","    def _calc(self, h_re, h_im, t_re, t_im, r_re, r_im):\n","        return torch.sum(\n","            h_re * t_re * r_re\n","            + h_im * t_im * r_re\n","            + h_re * t_im * r_im\n","            - h_im * t_re * r_im,\n","            -1\n","        )\n","\n","\n","    def forward(self, h, r, t, n):\n","        h_re = self.ent_re_embeddings(h)\n","        h_im = self.ent_im_embeddings(h)\n","        t_re = self.ent_re_embeddings(t)\n","        t_im = self.ent_im_embeddings(t)\n","        r_re = self.rel_re_embeddings(r)\n","        r_im = self.rel_im_embeddings(r)\n","        n_re = self.ent_re_embeddings(n)\n","        n_im = self.ent_im_embeddings(n)\n","        pos_score = self._calc(h_re, h_im, t_re, t_im, r_re, r_im)\n","        neg_score = self._calc(n_re, n_im, t_re, t_im, r_re, r_im)\n","        neg_score_tail = self._calc(h_re, h_im, t_re, t_im, n_re, n_im)\n","\n","        return pos_scorem, neg_score, neg_score_tail\n","\n","\n","    def regularization(self, data):\n","        batch_h = data['batch_h']\n","        batch_t = data['batch_t']\n","        batch_r = data['batch_r']\n","        h_re = self.ent_re_embeddings(batch_h)\n","        h_im = self.ent_im_embeddings(batch_h)\n","        t_re = self.ent_re_embeddings(batch_t)\n","        t_im = self.ent_im_embeddings(batch_t)\n","        r_re = self.rel_re_embeddings(batch_r)\n","        r_im = self.rel_im_embeddings(batch_r)\n","        regul = (torch.mean(h_re ** 2) + \n","                 torch.mean(h_im ** 2) + \n","                 torch.mean(t_re ** 2) +\n","                 torch.mean(t_im ** 2) +\n","                 torch.mean(r_re ** 2) +\n","                 torch.mean(r_im ** 2)) / 6\n","        \n","        return regul\n","\n","\n","    def predict(self, data):\n","        score = -self.forward(data)\n","        return score.cpu().data.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch, os\n","import json\n","\n","\n","ent2id = dict()\n","id2ent = set()\n","rel2id = dict()\n","id2rel = set()\n","with open('train.txt', 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        line = line.split('\\t')\n","        id2ent.add(line[0])\n","        id2rel.add(line[1])\n","        id2ent.add(line[2])\n","\n","with open('valid.txt', 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        line = line.split('\\t')\n","        id2ent.add(line[0])\n","        id2rel.add(line[1])\n","        id2ent.add(line[2])\n","\n","with open('test.txt', 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        line = line.split('\\t')\n","        id2ent.add(line[0])\n","        id2rel.add(line[1])\n","        id2ent.add(line[2])\n","\n","id2ent = sorted(list(id2ent))\n","id2rel = sorted(list(id2rel))\n","\n","for i,meta in enumerate(id2ent):\n","    ent2id[meta] = i\n","\n","for i,meta in enumerate(id2rel):\n","    rel2id[meta] = i"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","import torch\n","import json\n","import numpy as np\n","\n","class DataSet(Dataset):\n","    def __init__(self, file_path):\n","        self.len = 0\n","        self.head = []\n","        self.rel = []\n","        self.tail = []\n","        self.triple = []\n","        self.negative = []\n","        self.ent2id = torch.load('ent2id.pt')\n","        self.id2ent = torch.load('id2ent.pt')\n","        self.rel2id = torch.load('rel2id.pt')\n","        self.id2rel = torch.load('id2rel.pt')\n","        self.ent_tot = len(self.id2ent)\n","        self.rel_tot = len(self.id2rel)\n","        with open(file_path) as f:\n","            for line in f:\n","                line = line.strip()\n","                line = line.split('\\t')\n","                self.len += 1\n","                self.head.append(int(line[0]))\n","                self.rel.append(int(line[1]))\n","                self.tail.append(int(line[2]))\n","                self.negative.append(np.random.randint(0, len(self.id2ent)))\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def __getitem__(self, idx):\n","        return self.head[idx], self.rel[idx], self.tail[idx], self.negative[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 0 23 17000 positive -> true score high\n","# 0 23 74000 negative -> true score negative"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main():\n","    opts = args\n","    print (\"load data ...\")\n","    train_data = DataSet('data/train2id.txt')\n","    train_loader = DataLoader(train_data, shuffle=True, batch_size=opts.batch_size)\n","    valid_data = DataSet('data/valid2id.txt')\n","    valid_loader = DataLoader(train_data, shuffle=True, batch_size=opts.batch_size)\n","\n","    print(\"save model...\")\n","    torch.save(model.state_dict(), 'kbgat.pt')\n","    print(\"[Saving embeddings of whole entities & relations...]\")\n","\n","    save_embeddings(model, opts, train_data.id2ent, train_data.id2rel)\n","    print(\"[Embedding results are saved successfully.]\")\n","\n","    print(\"load model ...\")\n","    model = TransE(opts, train_data.ent_tot, train_data.rel_tot)\n","    if opts.optimizer == 'Adam':\n","        optimizer = optim.Adam(model.parameters(), lr=opts.lr, weight_decay=opts.weight_decay)\n","    elif opts.optimizer == 'SGD':\n","        optimizer = optim.SGD(model.parameters(), lr=opts.lr)\n","    model.cuda()\n","    loss = nn.MarginRankingLoss(margin=opts.margin)\n","    loss.cuda()\n","    scheduler = torch.optim.lr_scheduler.StepLR(\n","        optimizer, step_size=500, gamma=0.5, last_epoch=-1)\n","\n","    print(\"start training\")\n","    for epoch in range(1, opts.epochs + 1):\n","        print(\"epoch : \" + str(epoch))\n","        model.train()\n","        epoch_start = time.time()\n","        epoch_loss = []\n","        tot = 0\n","        for i, batch_data in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            batch_h, batch_r, batch_t, batch_n = batch_data\n","            batch_h = torch.LongTensor(batch_h).cuda()\n","            batch_r = torch.LongTensor(batch_r).cuda()\n","            batch_t = torch.LongTensor(batch_t).cuda()\n","            batch_n = torch.LongTensor(batch_n).cuda()\n","            pos_score, neg_score = model(batch_h, batch_r, batch_t, batch_n)\n","            train_loss = loss(pos_score, neg_score, -torch.ones(pos_score.size(-1)).cuda())\n","            train_loss.backward()\n","            optimizer.step()\n","            batch_loss = train_loss.item()\n","            epoch_loss.append(batch_loss)\n","            tot += batch_h.size(0)\n","            print('\\r{:>10} epoch {} progress {} loss: {}\\n'.format('', epoch, tot / train_data.__len__(),\n","                                                                    train_loss.item()), end='')\n","        scheduler.step()\n","        end = time.time()\n","        time_used = end - epoch_start\n","        print('one epoch time: {} minutes'.format(time_used / 60))\n","        print('{} epochs'.format(epoch))\n","        print('epoch {} loss: {}'.format(epoch, sum(epoch_loss) / len(epoch_loss)))\n","\n","        with open('transe_log.txt', 'a') as f:\n","            f.write('loss : ' + str(sum(epoch_loss) / len(epoch_loss)) + '\\n')\n","\n","        if epoch % opts.save_step == 0:\n","            print(\"save model...\")\n","            torch.save(model.state_dict(), 'transe.pt')\n","\n","    print(\"save model...\")\n","    torch.save(model.state_dict(), 'transe.pt')\n","    print(\"[Saving embeddings of whole entities & relations...]\")\n","\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","import pickle\n","from typing import Any, Dict, List\n","\n","import jsonlines\n","import torch\n","import transformers\n","from pytorch_lightning import LightningModule\n","from pytorch_lightning.utilities import rank_zero_only\n","\n","import config\n","import genie.metrics as CustomMetrics\n","import os\n","from genie.constrained_generation import Trie, get_information_extraction_prefix_allowed_tokens_fn_hf\n","from genie.datamodule.utils import TripletUtils\n","from genie.models import GenieHF\n","\n","from .utils import label_smoothed_nll_loss # ?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log = general_utils.get_logger(__name__)\n","\n","\n","class GeniePL(LightningModule):\n","    \"\"\"\n","    A LightningModule organizes your PyTorch code into 5 sections:\n","        - Setup for all computations (init).\n","        - Train loop (training_step)\n","        - Validation loop (validation_step)\n","        - Test loop (test_step)\n","        - Optimizers (configure_optimizers)\n","    Read the docs:\n","        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n","    \"\"\"\n","\n","\n","    def __init__(self, hf_config=None, hparams_overrides=None, **kwargs):\n","        super().__init__()\n","\n","        # this line ensures params passed to LightningModule will be saved to ckpt\n","        # it also allows to access params with 'self.hparams' attribute\n","        self.save_hyperparameters()\n","\n","        if hparams_overrides is not None:\n","            # Overriding the hyper-parameters of a checkpoint at an arbitrary depth using a dict structure\n","            hparams_overrides = self.hparams.pop(\"hparams_overrides\")\n","            general_utils.update(self.hparams, hparams_overrides)\n","            log.info(\"Some values of the original hparams were overridden\")\n","            log.info(\"Hyper-parameters:\")\n","            log.info(self.hparams)\n","\n","        if self.hparams.hf_config is not None:\n","            # Initialization from a local, pre-trained GenIE PL checkpoint\n","\n","            if self.hparams.get(\"other_parameters\", None) is not None:\n","                self.hparams.hf_config.update(self.hparams.other_parameters)\n","\n","            self.model = GenieHF(self.hparams.hf_config)\n","\n","            assert self.hparams.get(\"tokenizer\", False) or self.hparams.get(\"tokenizer_path\", False), (\n","                \"If you initialize the model from a local checkpoint \"\n","                \"you need to either pass the tokenizer or the path to the tokenizer in the \"\n","                \"constructor \"\n","            )\n","\n","            if self.hparams.get(\"tokenizer\", False):\n","                self.tokenizer = self.hparams[\"tokenizer\"]\n","            else:\n","                self.tokenizer = transformers.BartTokenizer.from_pretrained(self.hparams.tokenizer_path)\n","                \n","        else:\n","            # Initialization from a HF model\n","            self.model, hf_config = GenieHF.from_pretrained(\n","                self.hparams.model_name_or_path,\n","                return_dict=True,\n","                other_parameters=self.hparams.get(\"other_parameters\", None),\n","            )\n","            self.tokenizer = transformers.BartTokenizer.from_pretrained(\n","                \"martinjosifoski/genie-rw\"\n","                if self.hparams.model_name_or_path == \"random\"\n","                else self.hparams.model_name_or_path\n","            )\n","            self.hparams.tokenizer = self.tokenizer  # Save in the checkpoint\n","            self.hparams.hf_config = hf_config  # Save in the checkpoint\n","\n","        log.info(\"HF model config:\")\n","        log.info(self.hparams.hf_config)\n","\n","        self.ts_precision = CustomMetrics.TSPrecision()\n","        self.ts_recall = CustomMetrics.TSRecall()\n","        self.ts_f1 = CustomMetrics.TSF1()\n","\n","        if not self.hparams.inference[\"free_generation\"]:\n","            self.entity_trie = Trie.load(self.hparams.inference[\"entity_trie_path\"])\n","            self.relation_trie = Trie.load(self.hparams.inference[\"relation_trie_path\"])\n","\n","        self.testing_output_parent_dir = kwargs.get(\"testing_output_parent_dir\", None)\n","\n","\n","    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None, **kwargs):\n","        output = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels,\n","            decoder_attention_mask=decoder_attention_mask,\n","            **kwargs,\n","        )\n","\n","        return output\n","\n","\n","    def process_batch(self, batch):\n","        if self.hparams.get(\"bos_as_first_token_generated\", True):\n","            return batch\n","\n","        # remove the starting bos token from the target\n","        batch[\"trg_input_ids\"] = batch[\"trg_input_ids\"][:, 1:]\n","        batch[\"trg_attention_mask\"] = batch[\"trg_attention_mask\"][:, 1:]\n","\n","        return batch\n","\n","\n","    def training_step(self, batch, batch_idx=None):\n","        batch = self.process_batch(batch)\n","\n","        model_output = self(\n","            input_ids=batch[\"src_input_ids\"],\n","            attention_mask=batch[\"src_attention_mask\"],\n","            labels=batch[\"trg_input_ids\"],\n","            decoder_attention_mask=batch[\"trg_attention_mask\"],\n","            use_cache=False,\n","        )\n","\n","        # the output from hf contains a loss term that can be used in training (see the function commented out above)\n","        logits = model_output.logits\n","\n","        # Note that pad_token_id used in trg_input_ids is 1, and not -100 used by the hugging face loss implementation\n","        loss, nll_loss = label_smoothed_nll_loss(\n","            logits.log_softmax(dim=-1),\n","            batch[\"trg_input_ids\"],\n","            batch[\"trg_attention_mask\"],\n","            epsilon=self.hparams.eps,\n","            ignore_index=self.tokenizer.pad_token_id,\n","        )\n","\n","        self.log(\"train-nll_loss\", nll_loss.item(), on_step=True, on_epoch=False, prog_bar=True)\n","\n","        return {\"loss\": loss}\n","\n","\n","    def validation_step(self, batch, batch_idx=None):\n","        batch = self.process_batch(batch)\n","\n","        model_output = self(\n","            input_ids=batch[\"src_input_ids\"],\n","            attention_mask=batch[\"src_attention_mask\"],\n","            labels=batch[\"trg_input_ids\"],\n","            decoder_attention_mask=batch[\"trg_attention_mask\"],\n","            use_cache=False,\n","        )\n","\n","        logits = model_output.logits\n","\n","        # Note that pad_token_id used in trg_input_ids is 1, and not -100 used by the hugging face loss implementation\n","        loss, nll_loss = label_smoothed_nll_loss(\n","            logits.log_softmax(dim=-1),\n","            batch[\"trg_input_ids\"],\n","            batch[\"trg_attention_mask\"],\n","            epsilon=self.hparams.eps,\n","            ignore_index=self.tokenizer.pad_token_id,\n","        )\n","\n","        self.log(\"val-nll_loss\", nll_loss.item(), on_step=False, on_epoch=True, prog_bar=True)\n","\n","        return {\"val-nll_loss\": nll_loss}\n","\n","\n","    def test_step(self, batch, batch_idx):\n","        raw_input = [sample[\"src\"] for sample in batch[\"raw\"]]\n","        raw_target = [sample[\"trg\"] for sample in batch[\"raw\"]]\n","        ids = [sample[\"id\"] for sample in batch[\"raw\"]]\n","\n","        # ==== Prediction related ===\n","\n","        # Generate predictions\n","        if self.hparams.inference[\"free_generation\"]:\n","            outputs = self.sample(\n","                batch,\n","                input_data_is_processed_batch=True,\n","                return_dict_in_generate=True,\n","                output_scores=True,\n","                testing=True,\n","                **self.hparams.inference[\"hf_generation_params\"],\n","            )\n","        else:\n","            prefix_allowed_tokens_fn = get_information_extraction_prefix_allowed_tokens_fn_hf(\n","                self,\n","                raw_input,\n","                bos_as_first_token_generated=self.hparams.get(\"bos_as_first_token_generated\", True),\n","                entities_trie=self.entity_trie,\n","                relations_trie=self.relation_trie,\n","            )\n","            outputs = self.sample(\n","                batch,\n","                input_data_is_processed_batch=True,\n","                return_dict_in_generate=True,\n","                output_scores=True,\n","                testing=True,\n","                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n","                **self.hparams.inference[\"hf_generation_params\"],\n","            )\n","\n","        preds = []\n","        for lpreds in outputs:\n","            # lpreds is a list of <= `num_return_sequences` predictions\n","            pred = None\n","\n","            if len(lpreds) > 0:\n","                score = lpreds[0][\"log_prob\"]\n","                if score != -1e9 and score != -math.inf:\n","                    pred = lpreds[0][\"text\"]\n","\n","            preds.append(pred)\n","\n","        return_object = {\"ids\": ids, \"inputs\": raw_input, \"targets\": raw_target, \"predictions\": preds}\n","\n","        if self.hparams.inference[\"save_testing_data\"] and self.hparams.inference[\"save_full_beams\"]:\n","            return_object[\"full_predictions\"] = outputs\n","\n","        self._write_testing_output(return_object)\n","\n","        return return_object\n","\n","\n","    def test_step_end(self, outputs: List[Any]):\n","        # Process the data in the format expected by the metrics\n","        predictions = [\n","            TripletUtils.convert_text_sequence_to_text_triples(\n","                text, verbose=self.hparams.inference[\"verbose_flag_in_convert_to_triple\"]\n","            )\n","            for text in outputs[\"predictions\"]\n","        ]\n","        targets = [\n","            TripletUtils.convert_text_sequence_to_text_triples(\n","                text, verbose=self.hparams.inference[\"verbose_flag_in_convert_to_triple\"]\n","            )\n","            for text in outputs[\"targets\"]\n","        ]\n","\n","        # Update the metrics\n","        p = self.ts_precision(predictions, targets)\n","        r = self.ts_recall(predictions, targets)\n","        f1 = self.ts_f1(predictions, targets)\n","\n","        # Log the loss\n","        self.log(\"test-precision_step\", p, on_step=True, on_epoch=False, prog_bar=True)\n","        self.log(\"test-recall_step\", r, on_step=True, on_epoch=False, prog_bar=True)\n","        self.log(\"test-f1_step\", f1, on_step=True, on_epoch=False, prog_bar=True)\n","\n","\n","    def _write_testing_output(self, step_output):\n","        output_path = f\"testing_output_{self.global_rank}.jsonl\"\n","\n","        if self.testing_output_parent_dir is not None:\n","            output_path = os.path.join(self.testing_output_parent_dir, output_path)\n","\n","        with jsonlines.open(output_path, \"a\") as writer:\n","            items = []\n","\n","            for i in range(len(step_output[\"predictions\"])):\n","                item_data = {\n","                    \"id\": step_output[\"ids\"][i],\n","                    \"input\": step_output[\"inputs\"][i],\n","                    \"target\": step_output[\"targets\"][i],\n","                    \"prediction\": step_output[\"predictions\"][i],\n","                }\n","\n","                if self.hparams.inference[\"save_testing_data\"] and self.hparams.inference[\"save_full_beams\"]:\n","                    item_data[\"full_prediction\"] = step_output[\"full_predictions\"][i]\n","\n","                items.append(item_data)\n","\n","            writer.write_all(items)\n","\n","\n","    @rank_zero_only\n","    def _write_testing_outputs(self, outputs):\n","        output_path = f\"testing_output.jsonl\"\n","\n","        if self.testing_output_parent_dir is not None:\n","            output_path = os.path.join(self.testing_output_parent_dir, output_path)\n","\n","        with jsonlines.open(output_path, \"w\") as writer:\n","            for process_output in outputs:\n","                for step_output in process_output:\n","                    items = []\n","\n","                    for i in range(len(step_output[\"predictions\"])):\n","                        item_data = {\n","                            \"id\": step_output[\"ids\"][i],\n","                            \"input\": step_output[\"inputs\"][i],\n","                            \"target\": step_output[\"targets\"][i],\n","                            \"prediction\": step_output[\"predictions\"][i],\n","                        }\n","\n","                        if self.hparams.inference[\"save_testing_data\"] and self.hparams.inference[\"save_full_beams\"]:\n","                            item_data[\"full_prediction\"] = step_output[\"full_predictions\"][i]\n","\n","                        items.append(item_data)\n","\n","                    writer.write_all(items)\n","\n","\n","    def test_epoch_end(self, outputs):\n","        \"\"\"Outputs is a list of either test_step outputs outputs\"\"\"\n","        # Log metrics aggregated across steps and processes (in ddp)\n","        self.log(\"test-precision\", self.ts_precision.compute())\n","        self.log(\"test-recall\", self.ts_recall.compute())\n","        self.log(\"test-f1\", self.ts_f1.compute())\n","\n","        if self.hparams.inference[\"save_testing_data\"]:\n","            # TODO: Can achieve the same result by collating the testing_output_{rank}.jsonl files\n","            if torch.distributed.is_initialized():\n","                torch.distributed.barrier()\n","                gather = [None] * torch.distributed.get_world_size()\n","                torch.distributed.all_gather_object(gather, outputs)\n","                # Gather is a list of `num_gpu` elements, each being the outputs object passed to the test_epoch_end\n","                outputs = gather\n","            else:\n","                outputs = [outputs]\n","\n","            self._write_testing_outputs(outputs)\n","\n","        return {\n","            \"test-acc\": self.ts_precision.compute(),\n","            \"test-recall\": self.ts_precision.compute(),\n","            \"test-f1\": self.ts_precision.compute(),\n","        }\n","\n","\n","    def configure_optimizers(self):\n","        # Apply weight decay to all parameters except for the biases and the weight for Layer Normalization\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","\n","        # Per-parameter optimization.\n","        # Each dict defines a parameter group and contains the list of parameters to be optimized in a key `params`\n","        # Other keys should match keyword arguments accepted by the optimizers and\n","        # will be used as optimization params for the parameter group\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparams.weight_decay,\n","                # \"betas\": self.hparams.adam_betas,\n","                \"betas\": (0.9, 0.999),\n","                \"eps\": self.hparams.adam_eps,\n","            },\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                # \"betas\": self.hparams.adam_betas,\n","                \"betas\": (0.9, 0.999),\n","                \"eps\": self.hparams.adam_eps,\n","            },\n","        ]\n","\n","        optimizer = torch.optim.AdamW(\n","            optimizer_grouped_parameters,\n","            lr=self.hparams.lr,\n","            weight_decay=self.hparams.weight_decay,\n","        )\n","\n","        if self.hparams.schedule_name == \"linear\":\n","            scheduler = transformers.get_linear_schedule_with_warmup(\n","                optimizer,\n","                num_warmup_steps=self.hparams.warmup_updates,\n","                num_training_steps=self.hparams.total_num_updates,\n","            )\n","        elif self.hparams.schedule_name == \"polynomial\":\n","            scheduler = transformers.get_polynomial_decay_schedule_with_warmup(\n","                optimizer,\n","                num_warmup_steps=self.hparams.warmup_updates,\n","                num_training_steps=self.hparams.total_num_updates,\n","                lr_end=self.hparams.lr_end,\n","            )\n","\n","        lr_dict = {\n","            \"scheduler\": scheduler,  # scheduler instance\n","            \"interval\": \"step\",  # The unit of the scheduler's step size. 'step' or 'epoch\n","            \"frequency\": 1,  # corresponds to updating the learning rate after every `frequency` epoch/step\n","            \"name\": f\"LearningRateScheduler-{self.hparams.schedule_name}\",  # Used by a LearningRateMonitor callback\n","        }\n","\n","        return [optimizer], [lr_dict]\n","\n","\n","    @staticmethod\n","    def _convert_surface_form_triplets_to_ids(triplets, entity_name2id, relation_name2id):\n","        triplets = [[entity_name2id[s], relation_name2id[r], entity_name2id[o]] for s, r, o in triplets]\n","\n","        return triplets\n","\n","\n","    @staticmethod\n","    def _convert_output_to_triplets(output_obj, entity_name2id, relation_name2id):\n","        if isinstance(output_obj[0], str):\n","            output = []\n","            for text in output_obj:\n","                triplets = TripletUtils.convert_text_sequence_to_text_triples(text)\n","\n","                if entity_name2id is not None and relation_name2id is not None:\n","                    triplets = GeniePL._convert_surface_form_triplets_to_ids(triplets, entity_name2id, relation_name2id)\n","\n","                output.append(triplets)\n","\n","            return output\n","\n","        for sample in output_obj:\n","            sample[\"textual_triplets\"] = TripletUtils.convert_text_sequence_to_text_triples(sample[\"text\"])\n","            if entity_name2id is not None and relation_name2id is not None:\n","                sample[\"id_triplets\"] = GeniePL._convert_surface_form_triplets_to_ids(\n","                    sample[\"textual_triplets\"], entity_name2id, relation_name2id\n","                )\n","\n","        return output_obj\n","\n","\n","    def sample(\n","        self,\n","        input_data,\n","        input_data_is_processed_batch=False,\n","        testing=False,\n","        seed=None,\n","        prefix_allowed_tokens_fn=None,\n","        entity_trie=None,\n","        relation_trie=None,\n","        convert_to_triplets=False,\n","        surface_form_mappings={\"entity_name2id\": None, \"relation_name2id\": None},\n","        **kwargs,\n","    ):\n","        training = self.training\n","        if training:\n","            self.eval()\n","\n","        \"\"\"Input data is a list of strings or a processed batch (contains src_input_ids,\n","        and src_attention_mask as expected in training)\"\"\"\n","        inference_parameters = self.hparams.inference[\"hf_generation_params\"].copy()\n","        inference_parameters.update(kwargs)\n","\n","        with torch.no_grad():\n","            # Get input_ids and attention masks\n","            if input_data_is_processed_batch:\n","                input_ids = input_data[\"src_input_ids\"]\n","                attention_mask = input_data[\"src_attention_mask\"]\n","                if prefix_allowed_tokens_fn is None and \"raw\" in input_data:\n","                    raw_input = [sample[\"src\"] for sample in input_data[\"raw\"]]\n","                else:\n","                    raw_input = None\n","            else:\n","                tokenizer_output = {\n","                    k: v.to(self.device)\n","                    for k, v in self.tokenizer(\n","                        input_data,\n","                        return_tensors=\"pt\",\n","                        padding=True,\n","                        max_length=self.hparams.max_input_length,\n","                        truncation=True,\n","                    ).items()\n","                }  # input_ids and attention_masks with `num_sentences x max_length` dims\n","                input_ids = tokenizer_output[\"input_ids\"]\n","                attention_mask = tokenizer_output[\"attention_mask\"]\n","                raw_input = input_data\n","\n","            # If an entity and relation prefix trie were passed, construct the corresponding constraining function\n","            if entity_trie is not None and relation_trie is not None:\n","                prefix_allowed_tokens_fn = get_information_extraction_prefix_allowed_tokens_fn_hf(\n","                    self,\n","                    raw_input,\n","                    bos_as_first_token_generated=self.hparams.get(\"bos_as_first_token_generated\", True),\n","                    entities_trie=entity_trie,\n","                    relations_trie=relation_trie,\n","                )\n","\n","            # Set the seed and generate the predictions\n","            if testing:\n","                transformers.trainer_utils.set_seed(self.hparams.inference[\"seed\"])\n","            elif seed is not None:\n","                transformers.trainer_utils.set_seed(seed)\n","\n","            output = self.model.generate(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                no_repeat_ngram_size=inference_parameters.pop(\"no_repeat_ngram_size\", 0),\n","                max_length=inference_parameters.pop(\"max_length\", self.hparams.max_output_length),\n","                early_stopping=inference_parameters.pop(\"early_stopping\", False),\n","                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n","                **inference_parameters,\n","            )\n","\n","            k = inference_parameters.get(\"num_return_sequences\", 1)\n","            # Process the output and construct a return object\n","            if inference_parameters.get(\"return_dict_in_generate\", False):\n","                output[\"sequences\"] = self.tokenizer.batch_decode(output[\"sequences\"], skip_special_tokens=True)\n","                output[\"sequences_scores\"] = output[\"sequences_scores\"].tolist()\n","\n","                assert len(output[\"sequences\"]) == len(output[\"sequences_scores\"])\n","\n","                batch = [\n","                    (output[\"sequences\"][i : i + k], output[\"sequences_scores\"][i : i + k])\n","                    for i in range(0, len(output[\"sequences\"]), k)\n","                ]\n","\n","                output = []\n","\n","                # Constructs the returned object and filters ill-formatted sequences\n","                for seqs, scores in batch:\n","                    output_obj = [\n","                        {\"text\": seq, \"log_prob\": score}\n","                        for seq, score in zip(seqs, scores)\n","                        # if score != -1e9 and score != -math.inf\n","                    ]\n","\n","                    if convert_to_triplets:\n","                        output_obj = GeniePL._convert_output_to_triplets(output_obj, **surface_form_mappings)\n","                        # for sample in output_obj:\n","                        #     sample['triplets'] = TripletUtils.convert_text_sequence_to_text_triples(sample['text'])\n","\n","                    output_obj = sorted(output_obj, key=lambda x: x[\"log_prob\"], reverse=True)\n","                    output.append(output_obj)\n","\n","                # returns a list of `num_sentences` lists\n","                # Where each inner list has `num_return_sequences` elements`\n","                # Where each dictionary has keys \"text\" and \"log_prob\" corresponding to a single predicted sequence\n","                # The elements in the list are sorted in descending order with respect to the log_prob\n","                return output\n","\n","            # Returns a list of `num_sentences` decoded (textual) sequences\n","            output = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n","            if convert_to_triplets:\n","                output = GeniePL._convert_output_to_triplets(output, **surface_form_mappings)\n","                # output = [TripletUtils.convert_text_sequence_to_text_triples(text) for text in output]\n","\n","            output = [output[i: i + k] for i in range(0, len(output), k)]\n","\n","            if training:\n","                self.train()\n","\n","            return output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import jsonlines\n","from torch.utils.data import Dataset\n","import config\n","from genie.datamodule.utils import TripletUtils\n","from tqdm import tqdm\n","\n","import genie.utils.general as utils\n","\n","\n","log = utils.get_logger(__name__)\n","\n","class Seq2SeqDataset(Dataset):\n","    def __init__(self, tokenizer, data, **kwargs):\n","        super().__init__()\n","        self.tokenizer = tokenizer\n","        self.data = data\n","        self.params = kwargs\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"id\": self.data[idx][0],\n","            \"src\": self.data[idx][1],\n","            \"trg\": self.data[idx][2],\n","        }\n","\n","    @classmethod\n","    def from_src_target_files(cls, tokenizer, data_dir, data_split, **kwargs):\n","        with open(os.path.join(data_dir, f\"{data_split}.source\")) as fs, open(\n","            os.path.join(data_dir, f\"{data_split}.target\")\n","        ) as ft:\n","            data = [(s.strip(), t.strip()) for s, t in zip(fs, ft)]\n","\n","        return cls(tokenizer, data, kwargs)\n","\n","\n","    def collate_fn(self, batch):\n","        \"\"\"batch is a list of samples retrieved with the above defined get function. We assume that the model generated the decoder_ids and any non-standard token processing on itself.\"\"\"\n","        collated_batch = {}\n","\n","        for attr_name in \"src\", \"trg\":\n","            if attr_name == \"src\":\n","                max_length = self.params[\"max_input_length\"]\n","\n","            elif attr_name == \"trg\":\n","                max_length = self.params[\"max_output_length\"]\n","            else:\n","                raise Exception(f\"Unexpected attribute name `{attr_name}`!\")\n","\n","            tokenizer_output = self.tokenizer(\n","                [sample[attr_name] for sample in batch],\n","                return_tensors=\"pt\",  # return PyTorch tensors\n","                return_attention_mask=True,\n","                padding=self.params[\"padding\"],\n","                max_length=max_length,\n","                truncation=self.params[\"truncation\"],\n","            )\n","\n","            for k, v in tokenizer_output.items():\n","                collated_batch[\"{}_{}\".format(attr_name, k)] = v\n","\n","        if self.params.get(\"target_padding_token_id\", None) is not None:\n","            trg_input_ids = collated_batch[\"trg_input_ids\"]\n","            trg_input_ids.masked_fill_(\n","                trg_input_ids == self.tokenizer.pad_token_id, self.params[\"target_padding_token_id\"]\n","            )\n","\n","        collated_batch[\"raw\"] = batch\n","\n","        return collated_batch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","    # check that lengths match, and that the start and end of tag tokens are the same\n","    l = []\n","    s = []\n","    e = []\n","    for n, c in full_codes.items():\n","        l.append(len(c))\n","        s.append(c[1])\n","        e.append(c[-2])\n","\n","    assert np.all(np.array(l) == l[0])\n","    assert np.all(np.array(s) == s[0])\n","    assert np.all(np.array(e) == e[0])\n","\n","    codes = {n: full_codes[n][2] for n in full_codes}\n","    tag_codes = set(codes[k] for k in codes)\n","\n","    codes[\"start_of_tag\"] = s[0]\n","    codes[\"end_of_tag\"] = e[0]\n","\n","    codes[\"EOS\"] = eos_token_id\n","    codes[\"BOS\"] = bos_token_id\n","\n","    status_codes = [\"ob\", \"s\", \"r\", \"o\"]\n","    status_next_token_name = [\"subject_token\", \"relation_token\", \"object_token\", \"end_of_entity_token\"]\n","\n","    if sentences is not None:\n","        sent_origs = [[codes[\"EOS\"]] + encode_fn(sent)[1:] for sent in sentences]\n","    else:\n","        sent_origs = []\n","\n","\n","    def get_status(sent):\n","        \"\"\"Returns the generation setting – mention generation, entity generation or outside\"\"\"\n","        status = 0\n","\n","        i = 0\n","        while i < len(sent) - 2:\n","            if sent[i] == codes[\"start_of_tag\"] and sent[i + 1] in tag_codes and sent[i + 2] == codes[\"end_of_tag\"]:\n","                status += 1\n","\n","            i += 1\n","\n","        status = status % 4\n","\n","        return status, status_codes[status]\n","\n","\n","    def get_last_tag_pointer(sent):\n","        \"\"\"Assumes that the last tag is fully generated i.e. <tag_name>\"\"\"\n","        i = len(sent) - 2\n","\n","        while i >= 0:\n","            if sent[i] == codes[\"start_of_tag\"] and sent[i + 1] in tag_codes and sent[i + 2] == codes[\"end_of_tag\"]:\n","                return i, i + 2\n","\n","            i -= 1\n","\n","        return None\n","\n","\n","    def prefix_allowed_tokens_fn(batch_id, sent):\n","        \"\"\"Sent is the thus far generated sequence of ids acting as output.\n","        Batch_id is the idx of the sentence that we are generating the output for.\"\"\"\n","        sent = sent.tolist()\n","\n","        # TODO: Figure out when and why the generation doesn't end after EOS is generated.\n","        # TODO: If the next two lines are removed, output contains many \"EOS EOS EOS...\" at the end.\n","        if len(sent) > 1 and sent[-1] == codes[\"EOS\"]:\n","            return []\n","\n","        # Force the generation of BOS as a first token to be generated\n","        # Necessary if the model is trained with [eos bos ... eos] as target\n","        if bos_as_first_token_generated and len(sent) == 1:\n","            return [codes[\"BOS\"]]\n","\n","        status, status_code = get_status(sent)\n","        if len(sent_origs) == 0:\n","            sent_orig = None\n","        else:\n","            sent_orig = sent_origs[batch_id]\n","\n","        # ---- IF inside a tag ----\n","        # return the next status tag if the start tag was generated last\n","        if len(sent) > 0 and sent[-1] == codes[\"start_of_tag\"]:\n","            return [codes[status_next_token_name[status]]]\n","\n","        # return closing tag if the start tag and the status tag have been generated\n","        if len(sent) > 1 and sent[-2] == codes[\"start_of_tag\"]:\n","            if sent[-1] in tag_codes:\n","                return [codes[\"end_of_tag\"]]\n","            else:\n","                return []\n","        # -------------------------\n","\n","        # ---- If outside of a tag ----\n","        # Get allowed tokens\n","        allowed_tokens = get_allowed_tokens(sent, sent_orig, status_code)\n","        \n","        return allowed_tokens\n","\n","\n","    def get_allowed_tokens(sent, sent_orig, status_code):\n","        if status_code == \"ob\":\n","            allowed_tokens = [codes[\"start_of_tag\"], codes[\"EOS\"]]\n","        elif status_code == \"s\":\n","            allowed_tokens = _get_allowed_tokens(sent, sent_orig, entities_trie)\n","        elif status_code == \"r\":\n","            allowed_tokens = _get_allowed_tokens(sent, sent_orig, relations_trie)\n","        elif status_code == \"o\":\n","            allowed_tokens = _get_allowed_tokens(sent, sent_orig, entities_trie)\n","        else:\n","            raise RuntimeError\n","\n","        return allowed_tokens\n","\n","\n","    def _get_allowed_tokens(sent, sent_orig, trie):\n","        pointer_start, pointer_end = get_last_tag_pointer(sent)\n","\n","        allowed_tokens = trie.get(sent[pointer_end + 1 :])\n","\n","        if codes[\"EOS\"] in allowed_tokens:\n","            allowed_tokens.remove(codes[\"EOS\"])\n","            allowed_tokens.append(codes[\"start_of_tag\"])\n","\n","        return allowed_tokens\n","\n","\n","    return prefix_allowed_tokens_fn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","import jsonlines\n","import pickle\n","import os\n","\n","\n","def get_trie_from_strings(\n","    string_iterable,\n","    add_leading_space_flag=True,\n","    remove_leading_bos=True,\n","    output_folder_path=None,\n","    trie_name=None,\n","    tokenizer=None,\n","):\n","    assert (output_folder_path is None and trie_name is None) or (\n","        output_folder_path is not None and trie_name is not None\n","    )\n","    from tqdm import tqdm\n","\n","    if tokenizer is None:\n","        from transformers import BartTokenizer\n","\n","        tokenizer = BartTokenizer.from_pretrained(\"martinjosifoski/genie-rw\")\n","\n","    if add_leading_space_flag:\n","        leading_space = lambda x: f\" {x}\"\n","    else:\n","        leading_space = lambda x: x\n","\n","    if remove_leading_bos:\n","        leading_bos = lambda x: x[1:]\n","    else:\n","        leading_bos = lambda x: x\n","\n","    encode_func = lambda x: leading_bos(tokenizer(leading_space(x))[\"input_ids\"])\n","    trie = Trie([encode_func(uniq_name) for uniq_name in tqdm(sorted(string_iterable))])\n","\n","    if output_folder_path is not None:\n","        trie.dump(output_folder_path=output_folder_path, file_name=trie_name, string_iterable=string_iterable)\n","\n","    return trie\n","\n","\n","\n","class Trie(object):\n","    def __init__(self, sequences):\n","        \"\"\"sequences is a list of lists,\n","        each of which corresponds to a sequence of tokens encoded by the tokenizer\"\"\"\n","        next_sets = defaultdict(list)  # a dict that returns an empty list when the key is not in it\n","        for seq in sequences:\n","            if len(seq) > 0:\n","                next_sets[seq[0]].append(seq[1:])\n","\n","        self._leaves = {k: Trie(v) for k, v in next_sets.items()}\n","        # for the leaves of the trie _leaves == {}\n","\n","\n","    def get(self, indices):  # indices holds the list of vocabulary tokens that constitute the current prefix\n","        if len(indices) == 0:  # if we haven't generated anything so far: return all possible starting tokens\n","            return list(self._leaves.keys())\n","        elif indices[0] not in self._leaves:\n","            # if the currently leading token (and by extension the prefix) isn't eligible: return an empty list\n","            return []\n","        else:\n","            return self._leaves[indices[0]].get(indices[1:])  # take the trie that corresponds to the\n","\n","\n","    def dump(self, output_folder_path, file_name, string_iterable=None):\n","        pickle.dump(self, open(os.path.join(output_folder_path, f\"{file_name}.pickle\"), \"wb\"), protocol=4)\n","\n","        if string_iterable is not None:\n","            with jsonlines.open(os.path.join(output_folder_path, f\"{file_name}_original_strings.jsonl\"), \"w\") as writer:\n","                writer.write_all(string_iterable)\n","\n","\n","    @staticmethod\n","    def load(path):\n","        with open(path, \"rb\") as f:\n","            trie = pickle.load(f)\n","\n","        return trie"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02090842a8844674b3688ad0e15cfd30":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0663415a40a94c25b6ec736abc3c0130":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10f850375f6848d1b74e29563d9f5fd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23166b98c7434cc0b92ff0b76c9e55f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_876192d0308e4a7da22eb2922d98e360","max":2296723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10f850375f6848d1b74e29563d9f5fd0","value":2296723}},"2652885e6a23453f8079e721bcb89ebd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_30dbef9518c442618f9bce896b4305e7","max":14237,"min":0,"orientation":"horizontal","style":"IPY_MODEL_528331e3931d4a95af92d18657a034ab","value":14237}},"30dbef9518c442618f9bce896b4305e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bbec00911c8456ea250e3ed18e735d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f78d1274d93b4a3a8bca7a4d67c0df60","placeholder":"​","style":"IPY_MODEL_f72dcf6b56994814897c951f2ed7afc4","value":" 2.30M/2.30M [00:00&lt;00:00, 4.32MB/s]"}},"3ee48a4bbcc9470ea223b7c4601759bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9f49313698944a7acd149954708e786","placeholder":"​","style":"IPY_MODEL_0663415a40a94c25b6ec736abc3c0130","value":"Generating full split: 100%"}},"528331e3931d4a95af92d18657a034ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5591caf03ccb4e6caed4e3d193ce1ae4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84b2fa9a0a4049aeb4d90baeb1cd6b47","placeholder":"​","style":"IPY_MODEL_a61ffd51471f434f821c0d3b459f011e","value":"Downloading data: 100%"}},"6d196e6b2c46468caf90411fb892795f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5591caf03ccb4e6caed4e3d193ce1ae4","IPY_MODEL_23166b98c7434cc0b92ff0b76c9e55f5","IPY_MODEL_3bbec00911c8456ea250e3ed18e735d1"],"layout":"IPY_MODEL_02090842a8844674b3688ad0e15cfd30"}},"84b2fa9a0a4049aeb4d90baeb1cd6b47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8718b7f80f774f96bfccd64a94a4baee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ee48a4bbcc9470ea223b7c4601759bf","IPY_MODEL_2652885e6a23453f8079e721bcb89ebd","IPY_MODEL_eb145bec42f54dec83ea2ac4b556a477"],"layout":"IPY_MODEL_ee7617e52fdf482996972821edd749b9"}},"876192d0308e4a7da22eb2922d98e360":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a61ffd51471f434f821c0d3b459f011e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c544a43227064d4280870b5936a9cb62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5d557a5e36a4a0c896d420230fe645d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9f49313698944a7acd149954708e786":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb145bec42f54dec83ea2ac4b556a477":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c544a43227064d4280870b5936a9cb62","placeholder":"​","style":"IPY_MODEL_d5d557a5e36a4a0c896d420230fe645d","value":" 14237/14237 [00:00&lt;00:00, 9535.89 examples/s]"}},"ee7617e52fdf482996972821edd749b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f72dcf6b56994814897c951f2ed7afc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f78d1274d93b4a3a8bca7a4d67c0df60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
